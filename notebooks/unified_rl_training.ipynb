{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Reasoning RL Training\n",
    "\n",
    "This notebook implements a **two-phase training pipeline** for mathematical reasoning:\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "```\n",
    "Phase 1: SFT Cold Start (Required)\n",
    "    ‚îî‚îÄ‚îÄ Teaches model the <think>...</think> format\n",
    "    ‚îî‚îÄ‚îÄ Train for 1 epoch on format examples\n",
    "    ‚îî‚îÄ‚îÄ Saves to: checkpoints/sft/\n",
    "            ‚îÇ\n",
    "            ‚ñº\n",
    "Phase 2: RL Training (Choose Algorithm)\n",
    "    ‚îî‚îÄ‚îÄ Loads SFT checkpoint as base\n",
    "    ‚îî‚îÄ‚îÄ PPO / GRPO / Dr.GRPO / GSPO / DAPO / GRPO-LEAD\n",
    "    ‚îî‚îÄ‚îÄ Saves to: checkpoints/<algorithm>/\n",
    "```\n",
    "\n",
    "## Why SFT First?\n",
    "RL algorithms require the model to spontaneously emit `<think>` tags. An untrained model won't do this. SFT \"cold start\" teaches the format before RL optimizes for correctness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Training Configuration {display-mode: \"form\"}\n\n#@markdown ### Select Training Phase\nTRAINING_PHASE = \"Phase 1: SFT Cold Start\" #@param [\"Phase 1: SFT Cold Start\", \"Phase 2: RL Training\"]\n\n#@markdown ### RL Algorithm (only used in Phase 2)\n#@markdown **Recommended:** GRPO (stable baseline) or DR.GRPO (length-corrected)\nRL_ALGORITHM = \"GRPO\" #@param [\"PPO\", \"GRPO\", \"DR.GRPO\", \"GSPO\", \"DAPO\", \"GRPO-LEAD\"]\n\n#@markdown ---\n#@markdown ### Model & Data\nMODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\" #@param {type:\"string\"}\nDATASET_NAME = \"openai/gsm8k\" #@param [\"openai/gsm8k\", \"HuggingFaceH4/openr1-math-220k\"]\n\n#@markdown ---\n#@markdown ### Hyperparameters (Optimized for Performance)\n#@markdown **Memory Guide (with gradient checkpointing):**\n#@markdown - T4 (16GB): batch=1, group=2\n#@markdown - L4 (22GB): batch=1, group=4\n#@markdown - A100 40GB: batch=2, group=8\n#@markdown - A100 80GB: batch=8, group=8\nBATCH_SIZE = 8 #@param {type:\"integer\"}\nGROUP_SIZE = 8 #@param {type:\"integer\"}\nMAX_SAMPLES = 5000 #@param {type:\"integer\"}\nEPOCHS = 1 #@param {type:\"integer\"}\nSAVE_STEPS = 100 #@param {type:\"integer\"}\n\n#@markdown ### Phase-Specific Settings (auto-configured below)\nMAX_NEW_TOKENS = 384 #@param {type:\"integer\"}\nPPO_EPOCHS = 1 #@param {type:\"integer\"}\n\n#@markdown ---\n#@markdown ### Google Drive Settings\nPROJECT_NAME = \"unified-reasoning-rl\" #@param {type:\"string\"}\nRESUME_FROM_CHECKPOINT = True #@param {type:\"boolean\"}\n\n#@markdown ### Logging\nUSE_WANDB = False #@param {type:\"boolean\"}\nWANDB_PROJECT = \"UnifiedRL\" #@param {type:\"string\"}\n\n# ============================================================\n# AUTO-CONFIGURATION BASED ON PHASE\n# ============================================================\nIS_SFT_PHASE = \"SFT\" in TRAINING_PHASE\nALGORITHM = \"SFT\" if IS_SFT_PHASE else RL_ALGORITHM\n\n# Phase-specific learning rates (key for good performance!)\nif IS_SFT_PHASE:\n    LEARNING_RATE = 2e-5   # Higher LR for SFT (format learning is easy)\n    _MAX_NEW_TOKENS = 512  # Not used in SFT but set for reference\n    _PPO_EPOCHS = 1\nelse:\n    LEARNING_RATE = 5e-6   # Lower LR for RL (stability)\n    _MAX_NEW_TOKENS = MAX_NEW_TOKENS  # 384 recommended for reasoning\n    _PPO_EPOCHS = PPO_EPOCHS\n\n# Derived paths\nDRIVE_BASE_PATH = f\"/content/drive/MyDrive/Colab Notebooks/{PROJECT_NAME}\"\nSFT_CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/sft\"\nRL_CHECKPOINT_DIR = f\"{DRIVE_BASE_PATH}/checkpoints/{RL_ALGORITHM.lower().replace('.', '_')}\"\nCHECKPOINT_DIR = SFT_CHECKPOINT_DIR if IS_SFT_PHASE else RL_CHECKPOINT_DIR\n\n# ============================================================\n# DISPLAY CONFIGURATION\n# ============================================================\nprint(\"=\" * 60)\nprint(f\"üéØ TRAINING PHASE: {TRAINING_PHASE}\")\nprint(\"=\" * 60)\nprint(f\"Algorithm: {ALGORITHM}\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Dataset: {DATASET_NAME}\")\nprint(\"-\" * 60)\nprint(\"HYPERPARAMETERS:\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Batch Size: {BATCH_SIZE} | Group Size: {GROUP_SIZE}\")\nprint(f\"  Effective sequences/step: {BATCH_SIZE * GROUP_SIZE}\")\nprint(f\"  Max New Tokens: {_MAX_NEW_TOKENS}\")\nprint(f\"  PPO Epochs: {_PPO_EPOCHS}\")\nprint(f\"  Total Samples: {MAX_SAMPLES}\")\nprint(f\"  Training Epochs: {EPOCHS}\")\nprint(\"-\" * 60)\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")\nif not IS_SFT_PHASE:\n    print(f\"SFT Base: {SFT_CHECKPOINT_DIR}/final\")\nprint(\"=\" * 60)\n\n# Performance tips\nif IS_SFT_PHASE:\n    print(\"\\nüí° SFT Tips:\")\n    print(\"   ‚Ä¢ 1 epoch is usually enough (avoid overfitting)\")\n    print(\"   ‚Ä¢ Loss should decrease to ~1.5-2.5\")\n    print(\"   ‚Ä¢ If loss < 1.0, you may be overfitting\")\nelse:\n    print(\"\\nüí° RL Tips:\")\n    print(\"   ‚Ä¢ Watch accuracy - should increase over time\")\n    print(\"   ‚Ä¢ KL divergence should stay < 0.1\")\n    print(\"   ‚Ä¢ If accuracy stuck at 0%, check reward function\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
    "os.makedirs(SFT_CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RL_CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_BASE_PATH}/logs\", exist_ok=True)\n",
    "\n",
    "print(f\"Project directory: {DRIVE_BASE_PATH}\")\n",
    "print(f\"SFT checkpoints: {SFT_CHECKPOINT_DIR}\")\n",
    "print(f\"RL checkpoints: {RL_CHECKPOINT_DIR}\")\n",
    "\n",
    "# Check if SFT has been completed (for Phase 2)\n",
    "SFT_COMPLETED = os.path.exists(f\"{SFT_CHECKPOINT_DIR}/final\")\n",
    "if not IS_SFT_PHASE and not SFT_COMPLETED:\n",
    "    print(\"\\n\" + \"!\" * 60)\n",
    "    print(\"WARNING: SFT checkpoint not found!\")\n",
    "    print(\"Please run Phase 1 (SFT Cold Start) first.\")\n",
    "    print(\"!\" * 60)\n",
    "elif SFT_COMPLETED:\n",
    "    print(f\"\\n‚úì SFT checkpoint found: {SFT_CHECKPOINT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n# First, fix NumPy version (must be done before other installs)\n!pip uninstall numpy -y\n!pip install \"numpy<2.0.0\"\n\n# Install core dependencies\n!pip install -q torch torchvision torchaudio\n!pip install -q transformers>=4.40.0 peft>=0.10.0 accelerate>=0.30.0\n!pip install -q datasets>=2.18.0 scipy pyyaml tqdm wandb\n!pip install -q bitsandbytes>=0.43.0 safetensors\n\n# Flash Attention 2 (OPTIONAL - skip if you want to start training faster)\n# Uncomment the line below to install (takes ~15-20 min to compile on Colab)\n# !pip install flash-attn --no-build-isolation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU not available! Please enable GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Utility Functions {display-mode: \"form\"}\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_logger(name: str, log_dir: str = None):\n",
    "    \"\"\"Configures a standardized logger.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    if not logger.handlers:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "            fh = logging.FileHandler(os.path.join(log_dir, \"training.log\"))\n",
    "            fh.setFormatter(formatter)\n",
    "            logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir: str):\n",
    "    \"\"\"Find the latest checkpoint in a directory.\"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return None, 0\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, \"step_*\"))\n",
    "    if not checkpoints:\n",
    "        return None, 0\n",
    "    step_pattern = re.compile(r'step_(\\d+)')\n",
    "    steps = []\n",
    "    for ckpt in checkpoints:\n",
    "        match = step_pattern.search(ckpt)\n",
    "        if match:\n",
    "            steps.append((int(match.group(1)), ckpt))\n",
    "    if not steps:\n",
    "        return None, 0\n",
    "    steps.sort(reverse=True)\n",
    "    return steps[0][1], steps[0][0]\n",
    "\n",
    "print(\"‚úì Utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Dataset {display-mode: \"form\"}\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MathReasoningDataset(Dataset):\n",
    "    \"\"\"Wrapper for GSM8K/Math datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, split=\"train\", max_samples=None, mode=\"rl\", dataset_name=\"openai/gsm8k\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        if \"openr1\" in dataset_name.lower():\n",
    "            self.data = load_dataset(dataset_name, split=split)\n",
    "            self.problem_key = \"problem\"\n",
    "            self.solution_key = \"solution\"\n",
    "            self.answer_key = \"answer\"\n",
    "        elif \"gsm8k\" in dataset_name.lower():\n",
    "            self.data = load_dataset(dataset_name, \"main\", split=split)\n",
    "            self.problem_key = \"question\"\n",
    "            self.solution_key = \"answer\"\n",
    "            self.answer_key = \"answer\"\n",
    "        else:\n",
    "            self.data = load_dataset(dataset_name, split=split)\n",
    "            self.problem_key = \"problem\"\n",
    "            self.solution_key = \"solution\"\n",
    "            self.answer_key = \"answer\"\n",
    "\n",
    "        if max_samples:\n",
    "            max_samples = min(max_samples, len(self.data))\n",
    "            self.data = self.data.select(range(max_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _extract_gsm8k_answer(self, solution_text):\n",
    "        if \"####\" in solution_text:\n",
    "            return solution_text.split(\"####\")[-1].strip()\n",
    "        return solution_text.strip()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        problem = item.get(self.problem_key, \"\")\n",
    "\n",
    "        prompt = (\n",
    "            \"<|im_start|>system\\n\"\n",
    "            \"Please reason step by step and put your final answer within \\\\boxed{}.<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\"\n",
    "            f\"{problem}<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "            \"<think>\"\n",
    "        )\n",
    "\n",
    "        if self.mode == 'sft':\n",
    "            solution = item.get(self.solution_key, \"\")\n",
    "            # Format solution with think tags for SFT\n",
    "            full_text = prompt + \"\\n\" + solution + \"\\n</think>\\n\\\\boxed{\" + self._extract_gsm8k_answer(solution) + \"}<|im_end|>\"\n",
    "            return {\"text\": full_text}\n",
    "\n",
    "        answer = self._extract_gsm8k_answer(item.get(self.answer_key, \"\")) if \"gsm8k\" in self.dataset_name.lower() else item.get(self.answer_key, \"\")\n",
    "        return {\"prompt\": prompt, \"ground_truth\": answer}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if \"text\" in batch[0]:\n",
    "        return [b[\"text\"] for b in batch]\n",
    "    return {\"prompts\": [b[\"prompt\"] for b in batch], \"ground_truths\": [b[\"ground_truth\"] for b in batch]}\n",
    "\n",
    "print(\"‚úì Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4.3 Model {display-mode: \"form\"}\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n\n# Auto-detect Flash Attention\nFLASH_ATTN_AVAILABLE = False\ntry:\n    import flash_attn\n    FLASH_ATTN_AVAILABLE = True\n    print(\"‚úì Flash Attention 2 available\")\nexcept ImportError:\n    print(\"‚ö† Flash Attention not installed - using eager attention (slower)\")\n\nclass UnifiedPolicyModel(nn.Module):\n    \"\"\"Model with 4-bit quantization + LoRA using standard transformers + bitsandbytes.\"\"\"\n    \n    def __init__(self, model_name: str, algo: str, max_seq_length: int = 2048, load_in_4bit: bool = True):\n        super().__init__()\n        self.algo = algo.upper()\n        self.device = None\n        self.model_name = model_name\n\n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n        self.tokenizer.padding_side = \"left\"\n\n        # Auto-select attention implementation\n        attn_impl = \"flash_attention_2\" if FLASH_ATTN_AVAILABLE else \"eager\"\n        print(f\"Using attention: {attn_impl}\")\n\n        # 4-bit quantization config\n        if load_in_4bit:\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=True,\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                trust_remote_code=True,\n                attn_implementation=attn_impl,\n                torch_dtype=torch.bfloat16,\n            )\n            # Enable gradient checkpointing - required to fit in memory\n            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=True)\n            print(\"‚úì Gradient checkpointing enabled\")\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                trust_remote_code=True,\n                attn_implementation=attn_impl,\n            )\n            self.model.gradient_checkpointing_enable()\n\n        # Add LoRA\n        lora_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM,\n            r=16,\n            lora_alpha=32,\n            lora_dropout=0.0,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n            bias=\"none\",\n        )\n        self.model = get_peft_model(self.model, lora_config)\n        self.model.print_trainable_parameters()\n\n        # Critic head for PPO\n        self.critic = None\n        if self.algo == 'PPO':\n            hidden_size = self.model.config.hidden_size\n            self.critic = nn.Linear(hidden_size, 1).to(torch.bfloat16)\n\n    def to(self, device):\n        self.device = device\n        if self.critic is not None:\n            self.critic = self.critic.to(device)\n        return self\n\n    def forward(self, input_ids, attention_mask=None):\n        output_hidden = (self.critic is not None)\n        if attention_mask is None:\n            attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=output_hidden,\n            use_cache=False,\n        )\n        logits = outputs.logits\n        values = None\n        if self.critic is not None:\n            values = self.critic(outputs.hidden_states[-1]).squeeze(-1)\n        return logits, values\n\n    def generate(self, **kwargs):\n        return self.model.generate(**kwargs)\n\n    def save_pretrained(self, path):\n        os.makedirs(path, exist_ok=True)\n        self.model.save_pretrained(path)\n        self.tokenizer.save_pretrained(path)\n        if self.critic:\n            torch.save(self.critic.state_dict(), f\"{path}/critic.pt\")\n        with open(f\"{path}/training_info.txt\", \"w\") as f:\n            f.write(f\"algorithm: {self.algo}\\n\")\n            f.write(f\"base_model: {self.model_name}\\n\")\n\n    def load_adapter_from_checkpoint(self, path):\n        \"\"\"Load LoRA adapter weights from a checkpoint.\"\"\"\n        from peft import set_peft_model_state_dict\n        import safetensors.torch\n        \n        adapter_safetensors = os.path.join(path, \"adapter_model.safetensors\")\n        adapter_bin = os.path.join(path, \"adapter_model.bin\")\n        \n        if os.path.exists(adapter_safetensors):\n            state_dict = safetensors.torch.load_file(adapter_safetensors)\n            set_peft_model_state_dict(self.model, state_dict)\n            print(f\"‚úì Loaded adapter from {path}\")\n        elif os.path.exists(adapter_bin):\n            state_dict = torch.load(adapter_bin, map_location=\"cuda\")\n            set_peft_model_state_dict(self.model, state_dict)\n            print(f\"‚úì Loaded adapter from {path}\")\n        else:\n            print(f\"‚ö† No adapter found at {path}\")\n            \n        if self.critic and os.path.exists(f\"{path}/critic.pt\"):\n            self.critic.load_state_dict(torch.load(f\"{path}/critic.pt\", map_location=\"cuda\"))\n            print(f\"‚úì Loaded critic from {path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4.4 Trainer {display-mode: \"form\"}\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nimport re\nimport gc\n\nclass UnifiedReasoningTrainer:\n    def __init__(self, policy_model, config, device):\n        self.policy = policy_model\n        self.config = config\n        self.algo = config['algo'].upper()\n        self.G = config['group_size']\n        self.device = device\n        self.max_new_tokens = config.get('max_new_tokens', 384)\n        \n        self.optimizer = AdamW(self.policy.model.parameters(), lr=float(config['learning_rate']))\n        self.critic_optimizer = None\n        if self.policy.critic:\n            self.critic_optimizer = AdamW(self.policy.critic.parameters(), lr=1e-4)\n        self.ppo_epochs = config.get('ppo_epochs', 2)\n\n    def extract_answer(self, text):\n        match = re.search(r'\\\\boxed\\{', text)\n        if not match:\n            return \"\"\n        start_idx = match.end()\n        brace_count = 1\n        idx = start_idx\n        while idx < len(text) and brace_count > 0:\n            if text[idx] == '{': brace_count += 1\n            elif text[idx] == '}': brace_count -= 1\n            idx += 1\n        return text[start_idx:idx-1] if brace_count == 0 else \"\"\n\n    def compute_rewards(self, completions, ground_truths):\n        rewards = []\n        for comp, gt in zip(completions, ground_truths):\n            pred = self.extract_answer(comp).strip().replace(\" \", \"\")\n            gt_norm = str(gt).strip().replace(\" \", \"\")\n            rewards.append(1.0 if pred == gt_norm else 0.0)\n        return torch.tensor(rewards, device=self.device, dtype=torch.float32)\n\n    def compute_gae(self, rewards, values, gamma=0.99, lam=0.95):\n        advantages = torch.zeros_like(rewards)\n        last_gae = 0\n        for t in reversed(range(rewards.size(1))):\n            next_val = values[:, t + 1] if t + 1 < rewards.size(1) else 0.0\n            delta = rewards[:, t] + gamma * next_val - values[:, t]\n            last_gae = delta + gamma * lam * last_gae\n            advantages[:, t] = last_gae\n        return advantages, advantages + values\n\n    def compute_kl(self, log_probs, old_log_probs, mask):\n        return (0.5 * ((log_probs - old_log_probs) ** 2 * mask).sum() / mask.sum()).item()\n\n    def compute_entropy(self, logits, mask):\n        probs = F.softmax(logits, dim=-1)\n        ent = -(probs * F.log_softmax(logits, dim=-1)).sum(dim=-1)\n        return (ent * mask).sum().item() / mask.sum().item()\n\n    def loss_ppo(self, lp, olp, adv, ret, val, mask):\n        ratio = torch.exp(lp - olp)\n        s1, s2 = ratio * adv, torch.clamp(ratio, 0.8, 1.2) * adv\n        pl = -(torch.min(s1, s2) * mask).sum() / mask.sum()\n        vl = ((val - ret) ** 2 * mask).sum() / mask.sum()\n        return pl + 0.5 * vl\n\n    def loss_grpo(self, lp, olp, adv, mask):\n        ratio = torch.exp(lp - olp)\n        s1, s2 = ratio * adv, torch.clamp(ratio, 0.8, 1.2) * adv\n        return (-torch.min(s1, s2) * mask).sum() / mask.sum()\n\n    def loss_dr_grpo(self, lp, olp, adv, mask):\n        B = adv.shape[0] // self.G\n        lens = mask.sum(1).float().view(B, self.G)\n        scale = (lens / (lens.mean(1, keepdim=True) + 1e-6)).view(-1, 1).expand_as(lp)\n        ratio = torch.exp(lp - olp)\n        loss = -torch.min(ratio * adv, torch.clamp(ratio, 0.8, 1.2) * adv) * scale\n        return (loss * mask).sum() / mask.sum()\n\n    def loss_gspo(self, lp, olp, adv, mask):\n        ld = (lp - olp) * mask\n        rho = torch.exp(ld.sum(1) / (mask.sum(1) + 1e-6)).unsqueeze(-1)\n        return -torch.min(rho * adv, torch.clamp(rho, 0.8, 1.2) * adv).mean()\n\n    def loss_dapo(self, lp, olp, rg, adv, mask):\n        vm = rg.std(1) > 0\n        ratio = torch.exp(lp - olp)\n        upper = torch.where(adv > 0, 1.28, 1.20)\n        s2 = torch.clamp(ratio, 0.8, upper) * adv\n        loss = -torch.min(ratio * adv, s2) * mask\n        ve = vm.repeat_interleave(self.G).view(-1, 1).expand_as(loss)\n        if ve.sum() == 0: return torch.tensor(0.0, device=self.device, requires_grad=True)\n        return (loss * ve).sum() / (mask * ve).sum()\n\n    def loss_grpo_lead(self, lp, olp, rew, mask):\n        lens = mask.sum(1).float()\n        corr = rew == 1.0\n        if corr.any():\n            z = (lens - lens[corr].mean()) / (lens[corr].std() + 1e-6)\n            rew = torch.where(corr, rew * torch.exp(-0.1 * z.abs()), rew)\n        B = rew.shape[0] // self.G\n        rg = rew.view(B, self.G)\n        pr = rg.mean(1).repeat_interleave(self.G)\n        dw = (2.0 - pr).view(-1, 1).expand_as(lp)\n        adv = ((rg - rg.mean(1, keepdim=True)) / (rg.std(1, keepdim=True) + 1e-6)).view(-1, 1).expand_as(lp) * dw\n        ratio = torch.exp(lp - olp)\n        return (-torch.min(ratio * adv, torch.clamp(ratio, 0.8, 1.2) * adv) * mask).sum() / mask.sum()\n\n    def train_step(self, batch):\n        if self.algo == 'SFT':\n            texts = batch\n            inputs = self.policy.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(self.device)\n            out = self.policy.model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, use_cache=False)\n            logits = out.logits[..., :-1, :].clone()\n            labels = inputs.input_ids[..., 1:].clone()\n            am = inputs.attention_mask[..., 1:]\n            labels = labels.masked_fill(am == 0, -100)\n            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1), ignore_index=-100)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            return {'loss': loss.item(), 'reward': 0.0, 'kl_divergence': 0.0, 'entropy': 0.0, 'avg_response_length': 0.0, 'accuracy': 0.0}\n\n        prompts, gts = batch['prompts'], batch['ground_truths']\n        pe = [p for p in prompts for _ in range(self.G)]\n        ge = [g for g in gts for _ in range(self.G)]\n        inputs = self.policy.tokenizer(pe, return_tensors=\"pt\", padding=True, padding_side=\"left\").to(self.device)\n        \n        # Generation (no grad)\n        with torch.no_grad():\n            self.policy.model.eval()\n            outs = self.policy.generate(**inputs, max_new_tokens=self.max_new_tokens, do_sample=True, temperature=0.8, use_cache=True)\n        \n        pl = inputs.input_ids.shape[1]\n        cids = outs[:, pl:]\n        am = (cids != self.policy.tokenizer.pad_token_id).float()\n        dec = self.policy.tokenizer.batch_decode(cids, skip_special_tokens=True)\n        rew = self.compute_rewards(dec, ge)\n        \n        # Free generation cache\n        del inputs\n        torch.cuda.empty_cache()\n        \n        # Compute old log probs (no grad)\n        with torch.no_grad():\n            lg, ov = self.policy(outs)\n            lg = lg[:, pl-1:-1, :].clone()\n            olp = -F.cross_entropy(lg.reshape(-1, lg.size(-1)), cids.reshape(-1), reduction='none').view(cids.shape)\n            if ov is not None: ov = ov[:, pl-1:-1].clone()\n            del lg  # Free memory\n            torch.cuda.empty_cache()\n\n        B = rew.shape[0] // self.G\n        if self.algo == 'PPO':\n            sr = torch.zeros_like(olp)\n            li = am.sum(1).long() - 1\n            for i, idx in enumerate(li):\n                if idx >= 0: sr[i, idx] = rew[i]\n            adv, ret = self.compute_gae(sr, ov)\n            adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n        else:\n            rg = rew.view(B, self.G)\n            adv = ((rg - rg.mean(1, keepdim=True)) / (rg.std(1, keepdim=True) + 1e-6)).view(-1, 1).expand_as(olp)\n            ret = None\n\n        # Training loop\n        self.policy.model.train()\n        for _ in range(self.ppo_epochs):\n            lg, val = self.policy(outs)\n            lg = lg[:, pl-1:-1, :].clone()\n            if val is not None: val = val[:, pl-1:-1]\n            lp = -F.cross_entropy(lg.reshape(-1, lg.size(-1)), cids.reshape(-1), reduction='none').view(cids.shape)\n\n            if self.algo == 'PPO': loss = self.loss_ppo(lp, olp, adv, ret, val, am)\n            elif self.algo == 'GRPO': loss = self.loss_grpo(lp, olp, adv, am)\n            elif self.algo == 'DR.GRPO': loss = self.loss_dr_grpo(lp, olp, adv, am)\n            elif self.algo == 'GSPO': loss = self.loss_gspo(lp, olp, adv, am)\n            elif self.algo == 'DAPO': loss = self.loss_dapo(lp, olp, rew.view(B, self.G), adv, am)\n            elif self.algo == 'GRPO-LEAD': loss = self.loss_grpo_lead(lp, olp, rew, am)\n            else: raise ValueError(f\"Unknown: {self.algo}\")\n\n            self.optimizer.zero_grad()\n            if self.critic_optimizer: self.critic_optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.policy.model.parameters(), 1.0)\n            self.optimizer.step()\n            if self.critic_optimizer: self.critic_optimizer.step()\n            \n            # Free intermediate tensors\n            del lg\n            torch.cuda.empty_cache()\n\n        metrics = {'loss': loss.item(), 'reward': rew.mean().item(), 'kl_divergence': self.compute_kl(lp, olp, am),\n                   'entropy': 0.0, 'avg_response_length': am.sum(1).float().mean().item(),\n                   'accuracy': (rew == 1.0).float().mean().item()}\n        \n        # Cleanup\n        del outs, cids, olp, lp, adv, am, rew\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        return metrics\n\nprint(\"‚úì Trainer loaded (with memory optimization)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Phase 2 requirements\n",
    "if not IS_SFT_PHASE and not SFT_COMPLETED:\n",
    "    raise RuntimeError(\n",
    "        \"\\n\" + \"=\" * 60 + \"\\n\" +\n",
    "        \"ERROR: Cannot start RL training without SFT checkpoint!\\n\" +\n",
    "        \"Please run Phase 1 (SFT Cold Start) first.\\n\" +\n",
    "        \"Change TRAINING_PHASE to 'Phase 1: SFT Cold Start' and run again.\\n\" +\n",
    "        \"=\" * 60\n",
    "    )\n",
    "\n",
    "seed_everything(42)\n",
    "device = \"cuda\"\n",
    "logger = get_logger(\"Trainer\", f\"{DRIVE_BASE_PATH}/logs\")\n",
    "\n",
    "# Check for resume\n",
    "resume_path, resume_step = None, 0\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    resume_path, resume_step = find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "    if resume_path:\n",
    "        logger.info(f\"Found checkpoint at step {resume_step}\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Phase: {'SFT Cold Start' if IS_SFT_PHASE else 'RL Training'}\")\n",
    "print(f\"Algorithm: {ALGORITHM}\")\n",
    "print(f\"Resume from: {resume_path if resume_path else 'scratch'}\")\n",
    "print(f\"{'=' * 60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Model\nprint(f\"Loading model: {MODEL_NAME}...\")\npolicy = UnifiedPolicyModel(MODEL_NAME, ALGORITHM)\n# Device is handled by device_map=\"auto\", just set reference\npolicy.device = \"cuda\"\nif policy.critic:\n    policy.critic = policy.critic.to(\"cuda\")\n\n# For Phase 2: Load SFT checkpoint as base\nif not IS_SFT_PHASE:\n    sft_path = f\"{SFT_CHECKPOINT_DIR}/final\"\n    print(f\"Loading SFT weights from: {sft_path}\")\n    policy.load_adapter_from_checkpoint(sft_path)\n\n# Resume from checkpoint if available\nif resume_path:\n    print(f\"Resuming from: {resume_path}\")\n    policy.load_adapter_from_checkpoint(resume_path)\n\nprint(\"‚úì Model ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mode = 'sft' if IS_SFT_PHASE else 'rl'\n",
    "# Use smaller dataset for SFT (format tuning only needs ~5k samples)\n",
    "sft_samples = min(MAX_SAMPLES, 5000) if IS_SFT_PHASE else MAX_SAMPLES\n",
    "\n",
    "dataset = MathReasoningDataset(\n",
    "    policy.tokenizer,\n",
    "    max_samples=sft_samples,\n",
    "    mode=mode,\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Mode: {mode.upper()}\")\n",
    "print(f\"Samples: {len(dataset)}\")\n",
    "print(f\"Batches: {len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Trainer\nconfig = {\n    'algo': ALGORITHM,\n    'group_size': GROUP_SIZE if not IS_SFT_PHASE else 1,\n    'learning_rate': LEARNING_RATE,  # Auto-configured: 2e-5 for SFT, 5e-6 for RL\n    'ppo_epochs': _PPO_EPOCHS,\n    'max_new_tokens': _MAX_NEW_TOKENS,\n}\n\ntrainer = UnifiedReasoningTrainer(policy, config, device)\n\nif USE_WANDB:\n    import wandb\n    wandb.init(\n        project=WANDB_PROJECT, \n        name=f\"{ALGORITHM}_{MAX_SAMPLES}samples\",\n        config={\n            'phase': 'SFT' if IS_SFT_PHASE else 'RL', \n            'algorithm': ALGORITHM,\n            'model': MODEL_NAME,\n            'dataset': DATASET_NAME,\n            **config\n        }\n    )\n\nprint(\"‚úì Trainer initialized\")\nprint(f\"  Algorithm: {config['algo']}\")\nprint(f\"  Learning Rate: {config['learning_rate']}\")\nprint(f\"  Group Size: {config['group_size']}\")\nprint(f\"  Max New Tokens: {config['max_new_tokens']}\")\nprint(f\"  PPO Epochs: {config['ppo_epochs']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm.notebook import tqdm\nimport time\n\nstep = resume_step\nbest_metric = 0.0\nstart_time = time.time()\n\nphase_name = \"SFT Cold Start\" if IS_SFT_PHASE else f\"RL ({ALGORITHM})\"\nprint(f\"\\n{'=' * 60}\")\nprint(f\"üöÄ Starting {phase_name}\")\nprint(f\"{'=' * 60}\")\nprint(f\"Checkpoints: {CHECKPOINT_DIR}\")\nprint(f\"Total steps: {len(loader) * EPOCHS}\")\nprint(f\"{'=' * 60}\\n\")\n\nfor epoch in range(EPOCHS):\n    logger.info(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    pbar = tqdm(loader, desc=f\"Epoch {epoch + 1}\")\n    \n    epoch_metrics = {'loss': [], 'accuracy': [], 'kl': [], 'reward': []}\n    \n    for batch in pbar:\n        metrics = trainer.train_step(batch)\n        step += 1\n        \n        # Track metrics\n        epoch_metrics['loss'].append(metrics['loss'])\n        if not IS_SFT_PHASE:\n            epoch_metrics['accuracy'].append(metrics['accuracy'])\n            epoch_metrics['kl'].append(metrics['kl_divergence'])\n            epoch_metrics['reward'].append(metrics['reward'])\n        \n        # Progress bar\n        if IS_SFT_PHASE:\n            pbar.set_postfix({'loss': f\"{metrics['loss']:.4f}\"})\n        else:\n            pbar.set_postfix({\n                'loss': f\"{metrics['loss']:.4f}\",\n                'acc': f\"{metrics['accuracy']:.1%}\",\n                'reward': f\"{metrics['reward']:.2f}\",\n                'kl': f\"{metrics['kl_divergence']:.4f}\",\n            })\n        \n        if USE_WANDB:\n            wandb.log({**metrics, 'step': step, 'epoch': epoch + 1})\n        \n        # Save checkpoint\n        if step % SAVE_STEPS == 0:\n            ckpt = f\"{CHECKPOINT_DIR}/step_{step}\"\n            policy.save_pretrained(ckpt)\n            logger.info(f\"Saved: {ckpt}\")\n            \n            # Track best (lowest loss for SFT, highest accuracy for RL)\n            current = -metrics['loss'] if IS_SFT_PHASE else metrics['accuracy']\n            if current > best_metric:\n                best_metric = current\n                policy.save_pretrained(f\"{CHECKPOINT_DIR}/best\")\n                logger.info(f\"New best! {'Loss' if IS_SFT_PHASE else 'Accuracy'}: {abs(best_metric):.4f}\")\n    \n    # Epoch summary\n    print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n    print(f\"   Avg Loss: {sum(epoch_metrics['loss'])/len(epoch_metrics['loss']):.4f}\")\n    if not IS_SFT_PHASE:\n        print(f\"   Avg Accuracy: {sum(epoch_metrics['accuracy'])/len(epoch_metrics['accuracy']):.1%}\")\n        print(f\"   Avg Reward: {sum(epoch_metrics['reward'])/len(epoch_metrics['reward']):.3f}\")\n        print(f\"   Avg KL: {sum(epoch_metrics['kl'])/len(epoch_metrics['kl']):.4f}\")\n\n# Save final\npolicy.save_pretrained(f\"{CHECKPOINT_DIR}/final\")\nlogger.info(f\"Saved final: {CHECKPOINT_DIR}/final\")\n\nelapsed = time.time() - start_time\nif USE_WANDB:\n    wandb.finish()\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"‚úÖ {phase_name} Complete!\")\nprint(f\"{'=' * 60}\")\nprint(f\"Total time: {elapsed/60:.1f} minutes\")\nprint(f\"Final checkpoint: {CHECKPOINT_DIR}/final\")\nprint(f\"Best checkpoint: {CHECKPOINT_DIR}/best\")\nif IS_SFT_PHASE:\n    print(f\"\\nüëâ Next Step:\")\n    print(f\"   1. Change TRAINING_PHASE to 'Phase 2: RL Training'\")\n    print(f\"   2. Select RL_ALGORITHM (recommend: GRPO or DR.GRPO)\")\n    print(f\"   3. Run all cells again\")\nelse:\n    print(f\"\\nüëâ Next Step:\")\n    print(f\"   1. Run the Evaluation cell to test accuracy\")\n    print(f\"   2. Try different RL algorithms for comparison\")\nprint(f\"{'=' * 60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Evaluate Model {display-mode: \"form\"}\n\n#@markdown ### Evaluation Settings\n#@markdown - **Quick check**: 100 samples (~3-5 min)\n#@markdown - **Development**: 300 samples (~10 min)  \n#@markdown - **Full benchmark**: 1319 samples (~40-60 min)\nEVAL_SAMPLES = 300 #@param {type:\"integer\"}\nEVAL_TEMPERATURE = 0.0 #@param {type:\"number\"}\nEVAL_MAX_TOKENS = 512 #@param {type:\"integer\"}\n\nimport time\neval_start = time.time()\n\n# Load test set\neval_dataset = MathReasoningDataset(\n    policy.tokenizer, \n    split=\"test\", \n    max_samples=EVAL_SAMPLES, \n    mode='rl', \n    dataset_name=DATASET_NAME\n)\n\nprint(f\"{'=' * 50}\")\nprint(f\"üìä EVALUATION\")\nprint(f\"{'=' * 50}\")\nprint(f\"Samples: {len(eval_dataset)} / 1319 (GSM8K test set)\")\nprint(f\"Temperature: {EVAL_TEMPERATURE} ({'greedy' if EVAL_TEMPERATURE == 0 else 'sampling'})\")\nprint(f\"Max tokens: {EVAL_MAX_TOKENS}\")\nprint(f\"{'=' * 50}\\n\")\n\npolicy.model.eval()\ncorrect, total = 0, 0\nresults = []\n\nwith torch.no_grad():\n    for idx in tqdm(range(len(eval_dataset)), desc=\"Evaluating\"):\n        item = eval_dataset[idx]\n        inputs = policy.tokenizer(item['prompt'], return_tensors=\"pt\").to(device)\n        \n        if EVAL_TEMPERATURE == 0:\n            outputs = policy.generate(**inputs, max_new_tokens=EVAL_MAX_TOKENS, do_sample=False)\n        else:\n            outputs = policy.generate(**inputs, max_new_tokens=EVAL_MAX_TOKENS, do_sample=True, temperature=EVAL_TEMPERATURE)\n        \n        response = policy.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        pred = trainer.extract_answer(response).strip().replace(\" \", \"\")\n        gt = str(item['ground_truth']).strip().replace(\" \", \"\")\n        \n        is_correct = (pred == gt)\n        if is_correct:\n            correct += 1\n        total += 1\n        \n        results.append({\n            'idx': idx,\n            'correct': is_correct,\n            'pred': pred,\n            'gt': gt,\n        })\n\neval_time = time.time() - eval_start\naccuracy = correct / total\n\nprint(f\"\\n{'=' * 50}\")\nprint(f\"üìà RESULTS\")\nprint(f\"{'=' * 50}\")\nprint(f\"Accuracy: {accuracy:.1%} ({correct}/{total})\")\nprint(f\"Time: {eval_time/60:.1f} minutes\")\nprint(f\"{'=' * 50}\")\n\n# Show some examples\nprint(f\"\\nüìù Sample Results (first 5 incorrect):\")\nincorrect = [r for r in results if not r['correct']][:5]\nfor r in incorrect:\n    print(f\"   #{r['idx']}: pred='{r['pred']}' vs gt='{r['gt']}'\")\n\n# Extrapolate to full test set\nif EVAL_SAMPLES < 1319:\n    margin = 1.96 * ((accuracy * (1-accuracy) / total) ** 0.5)  # 95% CI\n    print(f\"\\nüìä Estimated full test accuracy: {accuracy:.1%} ¬± {margin:.1%} (95% CI)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Custom Problem {display-mode: \"form\"}\n",
    "\n",
    "PROBLEM = \"A store sells apples for $2 each. If you buy 5 apples and pay with a $20 bill, how much change do you get?\" #@param {type:\"string\"}\n",
    "\n",
    "prompt = f\"\"\"<|im_start|>system\n",
    "Please reason step by step and put your final answer within \\\\boxed{{}}.<|im_end|>\n",
    "<|im_start|>user\n",
    "{PROBLEM}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\"\"\"\n",
    "\n",
    "inputs = policy.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    out = policy.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "response = policy.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Problem:\", PROBLEM)\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Response:\")\n",
    "print(response.split(\"<think>\")[-1] if \"<think>\" in response else response)\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"Answer: {trainer.extract_answer(response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "print(\"=\" * 50)\n",
    "print(\"SAVED CHECKPOINTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for phase, path in [(\"SFT\", SFT_CHECKPOINT_DIR), (\"RL\", RL_CHECKPOINT_DIR)]:\n",
    "    print(f\"\\n{phase}: {path}\")\n",
    "    if os.path.exists(path):\n",
    "        items = sorted(os.listdir(path))\n",
    "        for item in items:\n",
    "            ip = os.path.join(path, item)\n",
    "            if os.path.isdir(ip):\n",
    "                size = sum(os.path.getsize(os.path.join(ip, f)) for f in os.listdir(ip) if os.path.isfile(os.path.join(ip, f)))\n",
    "                print(f\"  ‚îî‚îÄ {item}: {size/1e6:.1f} MB\")\n",
    "    else:\n",
    "        print(\"  (no checkpoints)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup GPU\n",
    "import gc\n",
    "del policy, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}